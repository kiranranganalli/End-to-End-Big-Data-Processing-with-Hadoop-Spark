# End-to-End Big Data Processing with Hadoop & Spark

## ğŸ“Œ Overview

This project demonstrates an **end-to-end big data analytics pipeline** that ingests large-scale web server log data, processes it using **Hadoop (HDFS)** and **Apache Spark**, transforms and aggregates the data using **SparkSQL** and **HiveQL**, and finally loads it into **Amazon Redshift** for BI reporting and dashboard creation.

The architecture is designed to handle **petabyte-scale data**, leverage **distributed computing** for performance, and support **real-time and batch analytical queries** for business decision-making.

---

## ğŸ¯ Objectives

1. **Ingest & Store** massive raw log data from multiple sources into **HDFS**.
2. **Process & Clean** the data using PySpark transformations for structured analysis.
3. **Enrich & Aggregate** data using SparkSQL & HiveQL.
4. **Model & Load** the cleaned data into **Amazon Redshift**.
5. **Visualize Insights** using dashboards (e.g., Tableau, QuickSight).

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    A[Raw Logs] -->|Ingestion| B[HDFS]
    B --> C[PySpark Processing on EMR]
    C --> D[Hive Table in Data Lake]
    D --> E[Amazon Redshift Data Warehouse]
    E --> F[BI Tools - Tableau/QuickSight]
```

**Components:**

* **HDFS** â€” Distributed storage for raw and processed data.
* **PySpark on EMR** â€” Scalable processing engine for data cleaning, transformation, and aggregation.
* **Hive Metastore** â€” Centralized schema repository for structured queries on HDFS data.
* **Amazon Redshift** â€” Columnar data warehouse for analytical queries.
* **BI Tools** â€” Visual dashboards to communicate results.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ ingestion_pipeline.py        # Hadoop ingestion logic
â”œâ”€â”€ spark_processing.py          # PySpark transformation & aggregation
â”œâ”€â”€ hive_queries.sql             # HiveQL scripts for table creation & queries
â”œâ”€â”€ redshift_loader.py           # ETL into Amazon Redshift
â”œâ”€â”€ sample_logs.csv               # 200+ rows sample dataset
â””â”€â”€ dashboards/                  # Exported Tableau/QuickSight dashboard files
```

---

## âš™ï¸ Data Flow

1. **Ingestion** â€” Logs ingested via **Flume** or **Kafka** and stored in **HDFS** partitions.
2. **Processing** â€” PySpark script cleans and parses logs into structured DataFrames.
3. **Aggregation** â€” SparkSQL & HiveQL queries compute metrics (e.g., daily active users, clickstream funnel).
4. **Load to Redshift** â€” Data is written to **S3** and then loaded into Redshift using `COPY` commands.
5. **Visualization** â€” Dashboards display KPIs like traffic sources, error rates, and regional activity.

---

## ğŸ› ï¸ Technologies Used

* **Apache Hadoop (HDFS)**
* **Apache Spark (PySpark, SparkSQL)**
* **Amazon EMR**
* **Apache Hive**
* **Amazon Redshift**
* **AWS S3**
* **Tableau / AWS QuickSight**
* **Python 3.x**
* **Boto3 (AWS SDK for Python)**

---

## ğŸ“Š Example Metrics Generated

* **Traffic Volume** â€” Page views, sessions, and unique visitors per hour/day/week.
* **Error Analysis** â€” HTTP error rates by type (404, 500).
* **Geo Analytics** â€” Traffic and performance by region.
* **Top Content** â€” Pages and categories generating the most engagement.

---

## ğŸš€ Deployment Steps

### 1ï¸âƒ£ Launch EMR Cluster

* Hadoop 3.x, Spark 3.x, Hive 3.x.
* Auto-scaling enabled.

### 2ï¸âƒ£ Data Ingestion

```bash
hdfs dfs -put sample_logs.csv /data/raw/
```

### 3ï¸âƒ£ Run Spark Processing

```bash
spark-submit spark_processing.py
```

### 4ï¸âƒ£ Execute Hive Queries

```bash
hive -f hive_queries.sql
```

### 5ï¸âƒ£ Load into Redshift

```bash
python redshift_loader.py
```

---

## ğŸ›¡ï¸ Data Quality & Governance

* **Schema Enforcement** â€” Hive Metastore ensures type consistency.
* **Partition Pruning** â€” Optimized query performance in Spark and Hive.
* **Data Validation** â€” Row count checks, null value monitoring before Redshift load.

---

## ğŸ“ˆ Dashboard Examples

1. **Traffic by Region** â€” Interactive map view.
2. **Error Trends** â€” Time-series breakdown of error rates.
3. **Top Performing Pages** â€” Sorted by engagement and conversions.

---

## ğŸ“Œ Next Steps

* Implement **real-time processing** with Spark Structured Streaming.
* Add **machine learning models** for anomaly detection.
* Integrate **data quality monitoring dashboards**.

---

## ğŸ‘¤ Author

**Kiran Ranganalli**
ğŸ“§ Email: [kiranranganalli@gmail.com](mailto:kiranranganalli@gmail.com)
ğŸ”— [LinkedIn](https://linkedin.com/in/kiranranganalli)
